<!doctype html>
<html>
<head>
	<meta charset="utf-8" />
    <meta name="description" content="Modibo Camara's website. I'm an Econ PhD student at Northwestern.">
    <meta name="author" content="Modibo Camara">
	<!--<link href="main.css" rel="stylesheet" />-->
	<title>Modibo Camara | PhD Student</title>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <link rel="stylesheet" href="https://latex.now.sh/style.css">
    <link rel="stylesheet" href="main.css">
</head>
<body>
    <a name="home"></a>
    <div>
    <header>
        <h1>Modibo Camara</h1>
        <!--<p>Here's a quick bio in lieu of a CV.</p>
        <ul>
            <li>I graduated in 2016 from the University of Pennsylvania, with majors in economics and math. I also have an M.A. in economics from Northwestern.</li>
            <li>I have done internships at Microsoft Research (2020), the Federal Reserve Board (2015), and the Commodity Futures Trading Commission (2014).</li>
            <li>I am grateful to be co-advised by Eddie Dekel (Econ) and Jason Hartline (CS).</li>
        </ul>-->
    </header>
    <nav>
        <ul list-style-type="none">
        <a href=#home><li>Home</li></a>
        <li>|</li>
        <a href=#papers><li>Working Papers</li></a>
        <li>|</li>
        <a href=#talks><li>Recorded Talks</li></a>
        </ul>
    </nav>
    <img class="photo" src="photo.jpg" width="38%" loading="lazy">
    <section>
        <p class="shift">Welcome! I'm a fifth-year Economics PhD student at <b>Northwestern University</b>.
            <ul>
                <li>Office: KGH 3368</li>
                <li>Email: <a href="mailto:mcamara@u.northwestern.edu">mcamara@u.northwestern.edu</a></li>
                <li>Twitter:  <a href="https://twitter.com/modibokhane">@modibokhane</a></li>
            </ul>
        </p>
        <p class="keep">I work at the intersection of <b>microeconomic theory</b>, <b>statistics</b>, and <b>computer science</b>.</p>

        <p>My research asks what people and institutions are capable of achieving, given the resources that are available to them -- resources like time and data.</p>

        <p>Ultimately, the goal for this line of research is to help people make better decisions and build better, more reliable institutions. This aligns with the view of economics as an engineering discipline.</p>
    </section>

    <section>
        <a name="papers"></a>
        <h2>Working Papers</h2>

        <h4>Hadwiger Separability, or: Turing meets Von Neumann and Morgenstern</h4>
        <h5>Previously known as <i>High-Dimensional Decision Theory</i> (<a href="high-dimensional-decision-theory.pdf">draft</a>).</h5>

        <p>
            I define a relaxation of additive separability, called Hadwiger separability. In a model of high-dimensional choice under uncertainty, I show that any rationalizable and computationally tractable choice correspondence has a Bernoulli utility function that is Hadwiger separable. The converse is also true, with caveats. This tractability axiom is used in computational complexity theory to identify problems that are too hard to solve, even for computers, regardless of which algorithm is used. I explore applications in behavioral economics and demand estimation. Finally, I reconsider whether choices reveal preferences, given computational constraints. Not necessarily; it turns out that for many natural (but intractable) utility functions, no optimal approximation algorithm is rationalizable.</p>

        <h4><a href="https://arxiv.org/abs/2009.05518">Mechanisms for a No-Regret Agent: Beyond the Common Prior</a> <!--(<a href="https://conference2.aau.at/event/4/contributions/173/">30-min talk</a>)</h4>-->

        <h5>Joint with <a href="https://sites.northwestern.edu/hartline/">Jason Hartline</a> and <a href="http://users.eecs.northwestern.edu/~acj861/">Aleck Johnsen</a>.
            </br>Forthcoming at FOCS 2020 (<a href="focs_camera.pdf">extended abstract</a>).</h5>

        <p>A rich class of mechanism design problems can be understood as incomplete-information games between a principal who commits to a policy and an agent who responds, with payoffs determined by an unknown state of the world. Traditionally, these models require strong and often-impractical assumptions about beliefs (a common prior over the state). In this paper, we dispense with the common prior. Instead, we consider a repeated interaction where both the principal and the agent may learn over time from the state history. We reformulate mechanism design as a reinforcement learning problem and develop mechanisms that attain natural benchmarks without any assumptions on the state-generating process. Our results make use of novel behavioral assumptions for the agent -- based on <i>counterfactual calibration</i> -- that capture the spirit of rationality without relying on beliefs.</p>
    </section>

    <section>
        <a name="talks"></a>
        <h2>Recorded Talks</h2>

        <h4>Mechanisms for a No-Regret Agent (24m)</h4>
        <p>Recorded in October 2020 for the FOCS conference. This was prepared for an audience of theoretical computer scientists, but should also be accessible to economists.</p>
        <center class="video">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Bt3IVL691Wg"></iframe>
        </center>
    </section>

    <footer>
        <br><hr>
        <p>This page was last modified on <span id="demo"></span>.</p>
        <script>
            var d = new Date(document.lastModified);
            document.getElementById("demo").innerHTML = d.toDateString();
        </script>
    </footer>

    <div id="all"><!--width="300"-->

        

        <!--<div id="nav"></div>
        <script>
        $("#nav").load("nav.html");
        </script>-->
       
        <div id="main">

            


            <a name="about"></a>
            

            <!-- <p>Previously, I was an undergraduate at the University of Pennsylvania (majors in econ and math, minor in compsci). I spent two incredible summers at the Federal Reserve Board and the Commodity Futures Trading Commission.</p>

            <p>My dissertation committee includes Eddie Dekel (co-chair), Jeff Ely, Jason Hartline (co-chair), and Marciano Siniscalchi. But all weird opinions are my own.</p>

            <p>Along with Lorenzo Stanca and Xiaoyu Cheng, I occasionally organize a reading group in economic theory. If you want to join the mailing list or want to present something, shoot me an email!</p>

            <p>Teaching: ugrad economics of medicine, applied econometrics, intermediate micro II; grad econometrics II, econometrics III. RAing for CET and Eddie.</p> -->
            <a name="papers"></a>
            <div class="divider"></div>
            <div>
                

            </div>

            <a name="research"></a>
            <div class="divider"></div>

            <!--<div>
                <h3>Dissertation Research</h3>

                <p>Economists predict behavior using a simple identifying assumption: that economic agents act in their own best interest. We tend to define "best interest" narrowly enough to derive useful conclusions from a given model, but broadly enough to include certain behaviors that we see empirically. Along the way, we acknowledge that many of our models assume a level of competence that we ourselves would find difficult to satisfy. But only rarely do we take advantage of the fact that statisticians and computer scientists have spent decades formalizing the ways in which decision problems are difficult, and characterizing the difficulty that state-of-the-art methods can handle.</p>

                <p>So: can we leverage theories of complexity to weaken or strengthen our behavioral assumptions in a way that (a) makes our theory more credible, where needed, (b) justifies existing theory, where possible, and (c) modifies our predictions appropriately as a problem becomes more complicated? Can these revised models explain real-world phenomena that are difficult or impossible to express via existing models? My dissertation research answers these questions affirmatively.</p>

                <p>In particular, my work uses a three-step methodology to revise classic models in mechanism design and decision theory. It is easy to describe in the abstract (although the execution can be more intricate). Given an existing model:</p>
                <ol>
                    <li>Figure out what we're asking agents to do and how real-world experts would accomplish that task.</li>
                    <li>Identify the performance benchmarks that real-world experts can guarantee (resp. that are impossible to achieve).</li>
                    <li>Assume that agents weakly overperform (resp. strictly underperform) those benchmarks.</li>
                </ol>
            </div>


            <a name="thoughts"></a>
            <div class="divider"></div>
            <div>
                <h3>Miscellaneous</h3>
                <p>I'll occasionally write about topics that I'm interested in and/or opinions that I'd like to express. See the links below.</p>
                <ul>
                    <li><a href="social-engineering.html">A Path Towards Social Engineering</a></li>
                    <li><a href="partial-choice.html">Observability Issues in Decision Theory</a></li>
                </ul>
                <p>Please feel free to reach out if anything interests you.</p>
            </div>-->
        </div>
    </div>
</div>
</body>
</html>